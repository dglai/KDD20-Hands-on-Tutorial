{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic operations on distributed data structure\n",
    "\n",
    "This notebook shows the operations on three distributed data structures: DistGraph, DistTensor and DistEmbedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DGL's distributed module\n",
    "\n",
    "`initialize` has to be called before calling any DGL's distributed API. Users need to at least provide the IP configuration of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl.distributed.initialize('ip_config.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DistGraph\n",
    "\n",
    "When creating a DistGraph object, it will load the input graph or connected to the servers that load the input graph, depending on its execution mode. `conf_file` is only required for the standalone mode.\n",
    "\n",
    "The code below loads the OGB product graph that was partitioned with Metis. When running in the standalone mode, the input graph can only have one partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.distributed.DistGraph('ogbn-products', part_config='standalone_data/ogbn-products.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access DistGraph\n",
    "\n",
    "We can access some basic information of the input graph structure from DistGraph, e.g., the number of nodes and the number of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#nodes:', g.number_of_nodes())\n",
    "print('#edges:', g.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input graph contains features and labels on nodes. These features and labels as well as the mask arrays (`train_mask`, `val_mask` and `test_mask`) are loaded into memory and shown up in `g.ndata` automatically.\n",
    "\n",
    "In addition, it contains `orig_id`. When a graph is partitioned, the node Ids and edge Ids are relabeled so that all node Ids and edge Ids in a partition fall in a contiguous range. The original node Ids and edge Ids are stored as node data and edge data of `orig_id` in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(g.ndata.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_mask` indicates whether a node belongs to the training set. The data is stored in a distributed tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.ndata['train_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the data in the distributed tensor, we need to explicitly slice data from it. The slicing operation copies the data to the local process. The sliced data is stored in a Pytorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.ndata['train_mask'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the values of `orig_id`. We are running it in the standalone mode, the node Ids were not relabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.ndata['orig_id'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the edge data has `orig_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(g.edata.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed tensors\n",
    "\n",
    "When a graph is loaded, all node data and edge data are loaded and stored as `DistTensor`. Normally, we don't need to create new distributed tensors during training. One use case of creating a new distributed tensor is in the inference stage where we want to store all intermediate node embeddings. We will see this example later in the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = dgl.distributed.DistTensor((g.number_of_nodes(),), th.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the created tensor is initialized to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can customize the initialization by providing an initialization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(shape, dtype):\n",
    "    return th.rand(shape, dtype=dtype)\n",
    "arr = dgl.distributed.DistTensor((g.number_of_nodes(),), th.float32, init_func=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign a DistTensor as node data. DistGraph only allows DistTensor as node data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['new_data'] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed embeddings\n",
    "\n",
    "DGL provides `DistEmbedding` to help to train models with embeddings (e.g., DeepWalk). When the embeddings are updated by a mini-batch, only the embeddings involved in the mini-batch are updated. As such, we can use `DistEmbedding` to train very large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = dgl.distributed.DistEmbedding(g.number_of_nodes(), 10, init_func=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL provides a sparse optimizer for `DistEmbedding`. For example, a user can use `SparseAdagrad` to update the embeddings. The tensor returned from `DistEmbedding` is attached with gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = dgl.distributed.SparseAdagrad([emb], lr=0.001)\n",
    "feats = emb([0,1,2,3])\n",
    "print(feats)\n",
    "loss = th.sum(feats + 1)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When getting data from `DistEmbedding` without recording gradients, no gradients are attached to the returned tensor. We can also see the embeddings for node 0, 1, 2, 3 have been updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    print(emb([0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
