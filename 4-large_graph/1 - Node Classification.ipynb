{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Training of GNN for Node Classification on Large Graphs\n",
    "\n",
    "This tutorial shows how to train a multi-layer GraphSAGE for node classification on Amazon Copurchase Network provided by OGB.  The dataset contains 2.4 million nodes and 61 million edges, hence not fitting in a single GPU.\n",
    "\n",
    "This tutorial's contents include\n",
    "\n",
    "* Creating your DGL graph from your own data in other formats such as CSV.\n",
    "* Training a GNN model with a single machine, a single GPU, on a graph of any size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Although you can directly use the Python package provided by OGB, for demonstration, we will instead manually download the dataset, peek into its contents, and process it with only `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget https://snap.stanford.edu/ogb/data/nodeproppred/products.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o products.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains these files:\n",
    "\n",
    "* `products/raw/edge.csv` (source-destination pairs)\n",
    "* `products/raw/node-feat.csv` (node features)\n",
    "* `products/raw/node-label.csv` (node labels)\n",
    "* `products/raw/num-edge-list.csv` (number of edges)\n",
    "* `products/raw/num-node-list.csv` (number of nodes)\n",
    "\n",
    "We will only use the first three CSV files.\n",
    "\n",
    "In addition, it also contains the following files defining the training-validation-test split in the directory `products/split/sales_ranking`.  All `train.csv`, `valid.csv` and `test.csv` are text files containing the node IDs in the training/validation/test set, one number per line.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> The node IDs should be consecutive integers from 0 to the number of nodes minus 1.  If your node ID is not consecutive or not starting from 0 (e.g., starting from 100000), you need to relabel them yourself.  The <code>astype</code> method in pandas DataFrame can conveniently relabel the IDs by converting the type to <code>\"category\"</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "edges = pd.read_csv('products/raw/edge.csv.gz', header=None).values\n",
    "node_features = pd.read_csv('products/raw/node-feat.csv.gz', header=None).values\n",
    "node_labels = pd.read_csv('products/raw/node-label.csv.gz', header=None).values[:, 0]\n",
    "\n",
    "# pd.read_csv yields a DataFrame with one column, so we make them one-dimensional arrays.\n",
    "train_nids = pd.read_csv('products/split/sales_ranking/train.csv.gz', header=None).values[:, 0]\n",
    "valid_nids = pd.read_csv('products/split/sales_ranking/valid.csv.gz', header=None).values[:, 0]\n",
    "test_nids = pd.read_csv('products/split/sales_ranking/test.csv.gz', header=None).values[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "graph = dgl.graph((edges[:, 0], edges[:, 1]))\n",
    "node_features = torch.FloatTensor(node_features)\n",
    "node_labels = torch.LongTensor(node_labels)\n",
    "\n",
    "# Save the graph, features and training-validation-test split for use for future tutorials.\n",
    "import pickle\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump((graph, node_features, node_labels, train_nids, valid_nids, test_nids), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph back from the file we saved\n",
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the size of the graph, features, and labels as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Graph')\n",
    "print(graph)\n",
    "print('Shape of node features:', node_features.shape)\n",
    "print('Shape of node labels:', node_labels.shape)\n",
    "\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Data Loader with Neighbor Sampling\n",
    "\n",
    "### Neighbor sampling overview\n",
    "\n",
    "The formulation of message passing usually has the following form:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "  \\boldsymbol{a}_v^{(l)} = \\rho^{(l)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_u^{(l-1)} : u \\in \\mathcal{N} \\left( v \\right)\n",
    "    \\right\\rbrace\n",
    "  \\right)\n",
    "\\\\\n",
    "  \\boldsymbol{h}_v^{(l)} = \\phi^{(l)} \\left(\n",
    "    \\boldsymbol{h}_v^{(l-1)}, \\boldsymbol{a}_v^{(l)}\n",
    "  \\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "where $\\rho^{(l)}$ and $\\phi^{(l)}$ are parameterized functions, and $\\mathcal{N}(v)$ represents the set of predecessors (or equivalently *neighbors*) of $v$ on graph $\\mathcal{G}$:\n",
    "$$\n",
    "\\mathcal{N} \\left( v \\right) = \\left\\lbrace\n",
    "  s \\left( e \\right) : e \\in \\mathbb{E}, t \\left( e \\right) = v\n",
    "\\right\\rbrace\n",
    "$$\n",
    "\n",
    "For instance, to perform a message passing for updating the red node in the following graph:\n",
    "\n",
    "![Imgur](assets/1.png)\n",
    "\n",
    "One needs to aggregate the node features of its neighbors, shown as green nodes:\n",
    "\n",
    "![Imgur](assets/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider how multi-layer message passing works for computing the output of a single node.  In the following text, we refer to the nodes whose outputs GNN will compute as seed nodes.\n",
    "\n",
    "Consider computing with a 2-layer GNN the output of the seed node 8, colored red, in the following graph:\n",
    "\n",
    "![Imgur](assets/seed.png)\n",
    "\n",
    "By the formulation:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "  \\boldsymbol{a}_8^{(2)} = \\rho^{(2)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_u^{(1)} : u \\in \\mathcal{N} \\left( 8 \\right)\n",
    "    \\right\\rbrace\n",
    "  \\right) = \\rho^{(2)} \\left(\n",
    "    \\left\\lbrace\n",
    "      \\boldsymbol{h}_4^{(1)}, \\boldsymbol{h}_5^{(1)},\n",
    "      \\boldsymbol{h}_7^{(1)}, \\boldsymbol{h}_{11}^{(1)}\n",
    "    \\right\\rbrace\n",
    "  \\right)\n",
    "\\\\\n",
    "  \\boldsymbol{h}_8^{(2)} = \\phi^{(2)} \\left(\n",
    "    \\boldsymbol{h}_8^{(1)}, \\boldsymbol{a}_8^{(2)}\n",
    "  \\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "We can tell from the formulation that, to compute $\\boldsymbol{h}_8^{(2)}$, we need messages from node 4, 5, 7, and 11 (colored green) along the edges visualized below.\n",
    "\n",
    "![Imgur](assets/3.png)\n",
    "\n",
    "The values of $\\boldsymbol{h}_\\cdot^{(1)}$ are the outputs from the first GNN layer.  To compute those values for the red and green nodes, we further need to perform message passing on the edges visualized below.\n",
    "\n",
    "![Imgur](assets/4.png)\n",
    "\n",
    "Therefore, to compute the 2-layer GNN representation of the red node, we need the input features from the red node as well as the green and yellow nodes.  Note that we should take red node's neighbors again for this layer.\n",
    "\n",
    "You may notice that the procedure which determines computation dependency is in the reverse direction of message aggregation: you start from the layer closest to the output and work backward to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see that computing representation for a small number of nodes often requires input features of a significantly larger number of nodes.  Taking all neighbors for message aggregation is often too costly since the nodes needed would easily cover a large portion of the graph.\n",
    "\n",
    "Neighbor sampling addresses this issue by selecting a random subset of the neighbors to perform aggregation.  For instance, to compute $\\boldsymbol{h}_8^{(1)}$, we can choose to sample 2 neighbors and aggregate.\n",
    "\n",
    "![Imgur](assets/5.png)\n",
    "\n",
    "Similarly, to compute the red and green nodes' first layer representation, we can also do neighbor sampling that takes 2 neighbors for each node.  Note that we should take the red node's neighbors again for this layer.\n",
    "\n",
    "![Imgur](assets/6.png)\n",
    "\n",
    "You can see that this method could give us fewer nodes needed for input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining neighbor sampler and data loader in DGL\n",
    "\n",
    "DGL provides useful tools to generate such computation dependencies while iterating over the dataset in minibatches.  For node classification, you can use `dgl.dataloading.NodeDataLoader` for iterating over the dataset, and `dgl.dataloading.MultiLayerNeighborSampler` to generate computation dependencies of the nodes from a multi-layer GNN with neighbor sampling.\n",
    "\n",
    "The syntax of `dgl.dataloading.NodeDataLoader` is mostly similar to a PyTorch `DataLoader`, with the addition that it needs a graph to generate computation dependency from, a set of node IDs to iterate on, and the neighbor sampler you defined.\n",
    "\n",
    "Let's consider training a 3-layer GraphSAGE with neighbor sampling, and each node will gather message from 4 neighbors on each layer.  The code defining the data loader and neighbor sampler will look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over the data loader we created and see what it gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NodeDataLoader` gives us three items per iteration.\n",
    "\n",
    "* The input node list for the nodes whose input features are needed to compute the outputs.\n",
    "* The output node list whose GNN representation are to be computed.\n",
    "* The list of computation dependency for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes, output_nodes, bipartites = example_minibatch\n",
    "print(\"To compute {} nodes' output we need {} nodes' input features\".format(len(output_nodes), len(input_nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `bipartites` shows how messages aggregate on each layer.  As its name suggests, it is a **list** of bipartite graphs.\n",
    "\n",
    "So why does DGL return a list of *bipartite* graphs for training a *homogeneous* graph?  The reason is that the number of nodes for input and that for output of a given GNN layer is different.  Take the example above:\n",
    "\n",
    "![Imgur](assets/6.png)\n",
    "\n",
    "That GNN layer will output the representation of three nodes (two green nodes and one red node), but it will require input from 7 nodes (the green nodes and red node, plus 4 yellow nodes).  Only a bipartite graph can describe such computation:\n",
    "\n",
    "![](assets/bipartite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch training of GNNs usually involves message passing on such bipartite graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bipartites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "\n",
    "The model can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that here we are iterating over the pairs of NN module layer and bipartite graphs generated by the data loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Loop\n",
    "\n",
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(num_features, 128, num_classes, 3).cuda()\n",
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the validation score for model selection, usually you can also do neighbor sampling.  To do that, you need to define another data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a training loop that performs validation every epoch.  It also saves the model with the best validation accuracy into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels = node_labels[output_nodes].cuda()\n",
    "            predictions = model(bipartites, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, bipartites in tq:\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels.append(node_labels[output_nodes].numpy())\n",
    "            predictions.append(model(bipartites, inputs).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Inference without Neighbor Sampling\n",
    "\n",
    "Usually for offline inference it is desirable to aggregate over the entire neighborhood to eliminate randomness introduced by neighbor sampling.  However, using the same methodology in training is not efficient, because there will be a lot of redundant computation.  Moreover, simply doing neighbor sampling by taking all neighbors will often exhaust GPU memory because the number of nodes required for input features may be too large to fit into GPU memory.\n",
    "\n",
    "Instead, you need to compute the representations layer by layer: you first compute the output of the first GNN layer for all nodes, then you compute the output of second GNN layer for all nodes using the first GNN layer's output as input, etc.  This gives us a different algorithm from what is being used in training.  During training we have an outer loop that iterates over the nodes, and an inner loop that iterates over the layers.  In contrast, during inference we have an outer loop that iterates over the layers, and an inner loop that iterates over the nodes.\n",
    "\n",
    "If you do not care about randomness too much (e.g., during model selection in validation), you can still use the `dgl.dataloading.MultiLayerNeighborSampler` and `dgl.dataloading.NodeDataLoader` to do offline inference, since it is usually faster for evaluating a small number of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](assets/anim.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = torch.arange(graph.number_of_nodes())\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            output_features = torch.zeros(\n",
    "                graph.number_of_nodes(), model.n_hidden if l != model.n_layers - 1 else model.n_classes)\n",
    "\n",
    "            for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "                bipartite = bipartites[0].to(torch.device('cuda'))\n",
    "\n",
    "                x = input_features[input_nodes].cuda()\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(bipartite, x)\n",
    "                if l != model.n_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "\n",
    "                output_features[output_nodes] = x.cpu()\n",
    "            input_features = output_features\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the best model from the file saved previously and performs offline inference.  It computes the accuracy on the test set afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "all_predictions = inference(model, graph, node_features, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = all_predictions[test_nids].argmax(1)\n",
    "test_labels = node_labels[test_nids]\n",
    "test_accuracy = sklearn.metrics.accuracy_score(test_predictions.numpy(), test_labels.numpy())\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE with neighbor sampling on a large dataset that cannot fit into GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "The next tutorial will be about training the same GraphSAGE model in an unsupervised manner with link prediction, i.e. predicting whether an edge exist between two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
